# ğŸš€ BOT GPT â€“ Conversational AI Backend

Production-grade conversational AI backend built with **FastAPI**, **SQLite**, and **Open-Source LLM integration (Ollama)**.

This project demonstrates:

- Clean backend architecture
- REST API design maturity
- Conversation persistence
- LLM integration
- Basic Retrieval-Augmented Generation (RAG)
- Cost-aware context management
- Docker + CI readiness

---

# ğŸ“Œ Overview

BOT GPT is a scalable conversational backend that supports:

## 1ï¸âƒ£ Open Chat Mode
- General LLM conversation
- Multi-turn chat
- Persistent conversation history
- Context window management

## 2ï¸âƒ£ Grounded Chat (RAG Mode)
- Chat over uploaded documents
- Text chunking
- Embedding generation
- FAISS vector retrieval
- Context injection into LLM prompt

---

# ğŸ— Architecture

```
Client (Postman / Swagger)
        |
        v
FastAPI (API Layer)
        |
        v
Service Layer
 - Conversation Service
 - LLM Service
 - RAG Service
        |
        v
Persistence Layer
 - SQLite (Users, Conversations, Messages)
 - FAISS (Vector Index)
        |
        v
Ollama (Llama3 - Local LLM)
```

---

# ğŸ›  Tech Stack

| Component | Technology |
|------------|------------|
| Backend | FastAPI |
| ORM | SQLAlchemy |
| Database | SQLite |
| LLM | Ollama (Llama3) |
| Embeddings | sentence-transformers |
| Vector Search | FAISS |
| Testing | Pytest |
| Containerization | Docker |

---

# ğŸ“‚ Project Structure

```
bot-gpt/
â”‚
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ database.py
â”‚   â”œâ”€â”€ models.py
â”‚   â”œâ”€â”€ schemas.py
â”‚   â”œâ”€â”€ config.py
â”‚   â”œâ”€â”€ routes/
â”‚   â”‚     â””â”€â”€ conversations.py
â”‚   â”œâ”€â”€ services/
â”‚   â”‚     â”œâ”€â”€ conversation_service.py
â”‚   â”‚     â”œâ”€â”€ llm_service.py
â”‚   â”‚     â””â”€â”€ rag_service.py
â”‚   â””â”€â”€ utils/
â”‚         â””â”€â”€ context_manager.py
â”‚
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_conversations.py
â”‚
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ Dockerfile
â””â”€â”€ README.md
```

---

# ğŸ—„ Data Model

## User
- id (UUID)
- email
- created_at

## Conversation
- id (UUID)
- user_id (FK)
- mode (open / rag)
- summary (optional)
- created_at

## Message
- id (UUID)
- conversation_id (FK)
- role (user / assistant / system)
- content
- created_at

Message ordering is maintained using timestamps.

---

# ğŸ”— API Endpoints

## Create Conversation

**POST** `/conversations`

Request:
```json
{
  "user_id": "123",
  "mode": "open",
  "first_message": "Hello AI"
}
```

Response:
```json
{
  "conversation_id": "uuid",
  "assistant_reply": "Hello! How can I help?"
}
```

---

## Continue Conversation

**POST** `/conversations/{id}/messages`

Request:
```json
{
  "message": "Explain transformers"
}
```

---

## Get Conversation Details

**GET** `/conversations/{id}`

---

## Delete Conversation

**DELETE** `/conversations/{id}`

---

# ğŸ§  Context Management Strategy

To prevent token overflow and reduce LLM cost:

- Sliding window (last N messages retained)
- Truncation before LLM call
- Optional summarization capability

This ensures scalability and cost efficiency.

---

# ğŸ“š RAG Flow

1. Document text is chunked.
2. Embeddings generated using sentence-transformers.
3. Stored in FAISS vector index.
4. On each user query:
   - Query embedding generated
   - Top-K relevant chunks retrieved
   - Context injected into LLM prompt

Prompt structure:

```
System: Use the provided context to answer the question.

Context:
<retrieved chunks>

User:
<question>
```

---

# âš™ï¸ Setup Instructions

## 1ï¸âƒ£ Clone Repository

```bash
git clone <your-repo-url>
cd bot-gpt
```

---

## 2ï¸âƒ£ Install Dependencies

```bash
pip install -r requirements.txt
```

---

## 3ï¸âƒ£ Install Ollama

Download and install from:

https://ollama.com/

Pull the model:

```bash
ollama pull llama3
```

Run once (to initialize):

```bash
ollama run llama3
```

---

## 4ï¸âƒ£ Run Backend

```bash
uvicorn app.main:app --reload
```

Open Swagger UI:

```
http://localhost:8000/docs
```

---

# ğŸ³ Docker Setup

Build Docker image:

```bash
docker build -t bot-gpt .
```

Run container:

```bash
docker run -p 8000:8000 bot-gpt
```

---

# ğŸ§ª Running Tests

```bash
pytest
```

---

# âš ï¸ Error Handling

Handled scenarios:

- LLM timeout â†’ graceful failure
- Invalid conversation ID â†’ 404 response
- Database transaction rollback
- Context truncation before token overflow

---

# ğŸ“ˆ Scalability Strategy

For scaling to high traffic (1M+ users):

### LLM Layer
- Async execution
- Background task queues (Celery + Redis)

### Database
- Migrate to PostgreSQL
- Add read replicas
- Connection pooling

### Vector Search
- Replace FAISS with Pinecone / Weaviate / Milvus

### Infrastructure
- Containerized services
- Horizontal scaling
- Load balancer
- Stateless API instances

---

# ğŸ”’ Production Improvements (Future Enhancements)

- JWT authentication
- Redis caching
- Streaming responses
- Rate limiting
- Structured logging
- Monitoring (Prometheus + Grafana)
- Cloud deployment (AWS / GCP)

---

# ğŸ¯ Evaluation Alignment

This implementation demonstrates:

- Clean modular backend design
- RESTful API maturity
- Data modeling clarity
- Real LLM API integration
- Basic RAG pipeline
- Context & cost awareness
- Docker & CI familiarity
- Production thinking mindset

---

# ğŸ‘¨â€ğŸ’» Author

**Shubham Jha**  
AI Engineer | Data Scientist  
India
